{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Decision Tree and Random Forests \n",
    "\n",
    "_Author and Instructor: **Dr. Junaid Qazi, PhD**_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi Guys,<br>\n",
    "\n",
    "Welcome to the Decision Tree and Random Forests lecture using scikit-learn in Python. <br>\n",
    "\n",
    "After learning key concepts on Decision Tree and Random Forests in the theory lecture, let's move on and use another famous dataset on [Heart Disease in Cleveland](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). This original and full dataset is a part of [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html) and contains 4 databases: ***Cleveland, Hungary, Switzerland, and the VA Long Beach.*** This dataset was donated to UCI Repository in 1988.<br>\n",
    "The original database contains 76 attributes, but all published experiments by machine learning researchers refer to using a subset of 14 of them. <br>\n",
    "In particular, the **Cleveland database** is the only one that has been used by the Machine Learning researchers to this date. In the original database, the \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).<br>\n",
    "\n",
    "We are also using Cleveland database in this section. You can download the original one from the UCI website or use the one provided along with this course. I recommend using the one provided in the curse material because it is already cleaned for the missing data. A new column 'target' is also added with N (for 0) and Y (for 1,2,3,4).<br>\n",
    "\n",
    "If you are interested to know more about the databases, please visit the link provided at the beginning.<br>\n",
    "The information on the 14 attribute, that we are going to use, is provided below: \n",
    "\n",
    "* **age**--in years\n",
    "* **sex**--(1 = male; 0 = female)\n",
    "* **cp**--chest pain type (1: typical angina, 2: atypical angina, 3: non-anginal, pain 4: asymptomatic)  \n",
    "* **trestbps** -- resting blood pressure\n",
    "* **chol**--serum cholesterol in mg/dl\n",
    "* **fbs**--fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n",
    "* **restecg**--resting ecg (electrocardiographic) results\n",
    "* **thalach**--maximum heart rate achieved \n",
    "* **exang**--exercise induced angina (1 = yes; 0 = no)\n",
    "* **oldpeak**--ST depression induced by exercise relative to rest \n",
    "* **slope**--the slope of the peak exercise ST segment (1: upsloping, 2: flat, 3: downsloping) \n",
    "* **ca**: number of major vessels (0-3) colored by flourosopy \n",
    "* **thal**: 3 = normal; 6 = fixed defect; 7 = reversable defect \n",
    "* **the predicted attribute** (0, 1, 2, 3 4) -- In the processed dataset, this one is added as a new column 'target' with 'N' for 0 and 'Y' for 1,2,3 & 4.<br>\n",
    "\n",
    "Let's move on to the jupyter notebook and learn by doing.\n",
    "\n",
    "#### Let's import the libraries and learn by doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data file in df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "df = pd.read_csv(\"HD_Cleveland_Data_Clean.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```Python\n",
    "df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "df.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "#some statistics you might be interested in!\n",
    "df.describe()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) -- only few plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "df['target'].value_counts()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "#If you are interested in pair plot, its big so I am just avoiding this!\n",
    "#sns.pairplot(data=df[['age','sex','chol','fbs','target']], hue='target')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```Python\n",
    "sns.countplot(x='target',data=df, hue='sex', palette='coolwarm')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the collected data, most of the men were diagnosed with the heart disease! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "#thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "sns.countplot(x='thal',data=df, palette='coolwarm')#, hue='sex')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Let's see how age is related to cholesterol, chol.\n",
    "sns.jointplot(x='age', y='chol', data=df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "```Python\n",
    "#thalach is the maximum heart rate achieved\n",
    "sns.jointplot(x='age', y='thalach', data=df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Section\n",
    "Our focus is Machine Learning, lets split the data and move on to the Machine Learning. **If you want, you can do more EDA so that you get even better understanding of your dataset.** <br>\n",
    "We will start with training a single decision tree and than compare the results with Random Forest but first, we need to do train test split!\n",
    "#### Train Test Split\n",
    "\n",
    "Let's split up the data into a training set and a test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Let's have a quick look at the colum names!\n",
    "df.columns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "X = df.drop('target',axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# the code below is same as above \n",
    "#X = df[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
    "#        'thalach','exang', 'oldpeak', 'slop', 'ca', 'thal']]\n",
    "#y = df['target']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# shift+tab and simply copy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Decision Trees\n",
    "\n",
    "We'll start with training a single decision tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# importing decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "#Creating instance \"dtree\" of the classifier \n",
    "dtree = DecisionTreeClassifier()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "#fitting to the training data, the default parameters are fine at the moment!\n",
    "dtree.fit(X_train,y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation \n",
    "\n",
    "Evaluation is important to see how did the model work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# doing predictions \n",
    "predictions = dtree.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "print(classification_report(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we are doing quite good using decision tree, the model is mislabeling some! <br><br><br>\n",
    "### Random Forests\n",
    "Let's try Random Forests model on the data and compare our results with the decision tree model. Random Forests is under ensemble class in the sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember theory lecture, we need to pass the number of trees in the forest which is n_estimators. The default is 10. <br>\n",
    "Let's pass 100 at the moment and fit the model to the training dataset.<br>&#9758;* You can play with n_estimators by changing different numbers!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Creating instance and fitting the model\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Follow [this link](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) if you want to learn more about the parameter in RandomForestClassifier. We have only passed the n_estimatores = 100 here and this is the one we frequently use. The default values for all other parameters are considered.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# doing predictions\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Evaluation\n",
    "print(classification_report(y_test,rfc_pred))\n",
    "print(confusion_matrix(y_test,rfc_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the random forest is gave improved results over a single tree for the dataset we have used. We got the better precision, recall and f1-score using Random Forest and less number of mislabeled samples!<br>\n",
    "You will see, if the dataset gets larger and larger, the Random Forests will always do better than a single decision tree. In the current situation, the data set is not very large but still Random Forests model works better than decision trees, the model will outshines with larger data sets. \n",
    "## Excellent work! \n",
    "So far, we have done great. Let's move on and do the project and practice our skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at the feature importance for both tree and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "df_feature_importance= pd.DataFrame(X_test.columns, columns = ['features'])\n",
    "df_feature_importance['feature_imp_Tree']=dtree.feature_importances_\n",
    "df_feature_importance['feature_imp_RF']=rfc.feature_importances_\n",
    "df_feature_importance.sort_values(by = ['feature_imp_Tree'], ascending = False, inplace = True)\n",
    "df_feature_importance.head(2)\n",
    "# if you want, you can set age as index column\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "df_feature_importance.plot(x='features', kind = 'bar', figsize = (16, 4))\n",
    "plt.ylabel('Importance')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Optional\n",
    "### Tree Visualization \n",
    "There is a built-in decision tree visualization capabilities in scikit learn. However, we don't use this most of the time because we often work with the Random Forests and it is much harder to visualize them because we have hundreds of trees rather than a single decision tree. We need to install pydot library for the tree visualization.<br>\n",
    "You need to install pydot using `pip install pydot` on terminal.<br>\n",
    "You also need `graphviz` library which is not actually the to display the image. Install both of these libraries and copy-paste the code from the reference note book to see how you tree looks like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import Image  \n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Lets separate the features \n",
    "features=df.columns[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "dot_data = StringIO()  \n",
    "export_graphviz(dtree, out_file=dot_data,\n",
    "                feature_names=features,filled=True,rounded=True)\n",
    "\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph[0].create_png())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# save the model to disk\n",
    "filename = 'final_model.sav'\n",
    "\n",
    "# rfc is our model, see above!\n",
    "# file will be stored on the disk, see the working directory\n",
    "pickle.dump(rfc, open(filename, 'wb')) # wb stands for reading only in binary format\n",
    "\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb')) # rb stands for reading only in binary format\n",
    "\n",
    "# let's do predictions using stored model after loading\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "# Let's pass the y_test and predictions to get the confusion_matrix\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
