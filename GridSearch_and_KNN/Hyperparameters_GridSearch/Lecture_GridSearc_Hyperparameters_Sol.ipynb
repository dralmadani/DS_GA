{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Hyperparameters and GridSearch\n",
    "\n",
    "_Dr. Junaid Qazi, PhD_\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand what the terms gridsearch and hyperparameter refer to.\n",
    "- Understand how to manually build a gridsearching procedure.\n",
    "- Apply sklearn's `GridSearchCV` object with basketball data to optimize a KNN model.\n",
    "- Practice using and evaluating attributes of the gridsearch object.\n",
    "- Understand the pitfalls of searching large hyperparameter spaces.\n",
    "- Practice the gridsearch procedure independently optimizing regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Lines below are just to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Picture: What part of the modelling process are we focusing on this morning?\n",
    "\n",
    "---\n",
    "\n",
    "As we have already explored, one general approach we can use for modelling questions would look like this:\n",
    "\n",
    "- STEP ONE: Cleaning, descriptive stats, correlation heatmap, plots & visualisations. Find baseline accuracy if it's a classifier.\n",
    "\n",
    "- STEP TWO: Set up predictor matrix (X) and target array (y).  Dummify if necessary.\n",
    "\n",
    "- STEP THREE: Train/test split and StandardScaler( )\n",
    "\n",
    "- **STEP FOUR:** Use cross-validation to optimise the hyperparameters for your model. You might try different types of models at this stage as well, and you might use GridSearchCV (or any of the other CVs like RidgeCV).\n",
    "\n",
    "- STEP FIVE: Once you're happy with your hyperparameters, fit your model on your whole training data and test it on your whole testing data.\n",
    "\n",
    "- STEP SIX: Then you might want to evaluate the performance of the model (R2 score, accuracy, confusion matrix, etc); find the actual predictions that your model is providing and store them in a dataframe; plot your predictions against your actual target variable to visualise how well your model performed; investigate feature importance with .coef_ if you have a parametric model.\n",
    "\n",
    "**Let's explore little more about STEP FOUR.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## What is \"Gridsearching\"? What are \"hyperparameters\"?\n",
    "\n",
    "---\n",
    "\n",
    "Models often have specifications that can be set. For example, when we choose a linear regression, we may decide to add a penalty to the loss function such as the Ridge or the Lasso. Those penalties require the regularization strength, alpha, to be set. \n",
    "\n",
    "**Model parameters are called hyperparameters.**\n",
    "\n",
    "Hyperparameters are different than the parameters of the model resulting from a fit, such as the coefficients. The **hyperparameters are set prior to the fit** and determine the behavior of the model.\n",
    "\n",
    "**There are often more than one kind of hyperparamter to set for a model.** For example, in the KNN algorithm, \n",
    "- we have a hyperparameter to set the **number of neighbors**. \n",
    "- We also have a hyperparameter for the **weights: uniform or distance**?\n",
    "\n",
    "We want to know the ***optimal*** hyperparameter settings, the set that results in the best model evaluation. \n",
    "\n",
    "**The search for the optimal set of hyperparameters is called gridsearching.**\n",
    "\n",
    "Gridsearching gets its name from the fact that we are searching over a **\"grid\" of parameters**. For example, imagine the `n_neighbors` hyperparameters on the x-axis and `weights` on the y-axis, and we need to test all points on the grid.\n",
    "\n",
    "**Gridsearching uses cross-validation internally to evaluate the performance of each set of hyperparameters.** More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basketball-data'></a>\n",
    "\n",
    "## Basketball data\n",
    "\n",
    "---\n",
    "\n",
    "To explore the process of gridsearching over sets of hyperparameters, we will use some basketball data. The data below has statistics for 4 different seasons of NBA basketball: 2013-2016.\n",
    "- This data includes aggregate **statistical data for each game**. \n",
    "- The data of each game is aggregated by match for all players.\n",
    "- Scraped from http://www.basketball-refrence.com\n",
    "\n",
    "Many of the columns in the dataset represent the mean of a statistic across the last 10 games, for example. Non-target statistics are for *prior* games, they do not include information about player performance in the current game.\n",
    "\n",
    "**We are interested in predicting whether the home team will win the game or not.** This is a classification problem.\n",
    "\n",
    "\n",
    "### Load the data and create the target and predictor matrix\n",
    "- The **target** will be a **binary column of whether the home team wins**.\n",
    "- The **predictors** should be **numeric** statistics columns.\n",
    "\n",
    "**Exclude these columns** from the predictor matrix:\n",
    "\n",
    "    ['GameId','GameDate','GameTime','HostName',\n",
    "     'GuestName','total_score','total_line','game_line',\n",
    "     'winner','loser','host_wins','Season']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### STEP ONE: Cleaning, descriptive stats, correlation heatmap, plots & visualisations. Find baseline accuracy if it's a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/basketball_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>GameId</th>\n",
       "      <th>GameDate</th>\n",
       "      <th>GameTime</th>\n",
       "      <th>HostName</th>\n",
       "      <th>GuestName</th>\n",
       "      <th>total_score</th>\n",
       "      <th>total_line</th>\n",
       "      <th>game_line</th>\n",
       "      <th>Host_HostRank</th>\n",
       "      <th>...</th>\n",
       "      <th>gPTS_avg10</th>\n",
       "      <th>gTS%_avg10</th>\n",
       "      <th>g3PAR_avg10</th>\n",
       "      <th>gFTr_avg10</th>\n",
       "      <th>gDRB%_avg10</th>\n",
       "      <th>gTRB%_avg10</th>\n",
       "      <th>gAST%_avg10</th>\n",
       "      <th>gSTL%_avg10</th>\n",
       "      <th>gBLK%_avg10</th>\n",
       "      <th>gDRtg_avg10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212090LAL</td>\n",
       "      <td>2012-12-09</td>\n",
       "      <td>6:30 pm</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>Utah Jazz</td>\n",
       "      <td>227.0</td>\n",
       "      <td>207.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>0.2230</td>\n",
       "      <td>0.2981</td>\n",
       "      <td>69.22</td>\n",
       "      <td>50.05</td>\n",
       "      <td>61.57</td>\n",
       "      <td>8.63</td>\n",
       "      <td>10.31</td>\n",
       "      <td>110.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212100PHI</td>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>201.0</td>\n",
       "      <td>186.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>90.3</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.3095</td>\n",
       "      <td>71.46</td>\n",
       "      <td>49.48</td>\n",
       "      <td>59.83</td>\n",
       "      <td>6.48</td>\n",
       "      <td>9.46</td>\n",
       "      <td>107.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212100HOU</td>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>240.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.5915</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.2518</td>\n",
       "      <td>74.26</td>\n",
       "      <td>50.99</td>\n",
       "      <td>61.82</td>\n",
       "      <td>8.30</td>\n",
       "      <td>6.85</td>\n",
       "      <td>101.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212110BRK</td>\n",
       "      <td>2012-12-11</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Brooklyn Nets</td>\n",
       "      <td>New York Knicks</td>\n",
       "      <td>197.0</td>\n",
       "      <td>195.5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>100.3</td>\n",
       "      <td>0.5473</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>0.2544</td>\n",
       "      <td>74.23</td>\n",
       "      <td>47.88</td>\n",
       "      <td>52.07</td>\n",
       "      <td>9.31</td>\n",
       "      <td>7.64</td>\n",
       "      <td>109.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212110DET</td>\n",
       "      <td>2012-12-11</td>\n",
       "      <td>7:30 pm</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>Denver Nuggets</td>\n",
       "      <td>195.0</td>\n",
       "      <td>203.5</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>101.1</td>\n",
       "      <td>0.5605</td>\n",
       "      <td>0.2173</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>68.45</td>\n",
       "      <td>50.40</td>\n",
       "      <td>56.33</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.83</td>\n",
       "      <td>114.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season        GameId    GameDate GameTime            HostName  \\\n",
       "0    2013  201212090LAL  2012-12-09  6:30 pm  Los Angeles Lakers   \n",
       "1    2013  201212100PHI  2012-12-10  7:00 pm  Philadelphia 76ers   \n",
       "2    2013  201212100HOU  2012-12-10  7:00 pm     Houston Rockets   \n",
       "3    2013  201212110BRK  2012-12-11  7:00 pm       Brooklyn Nets   \n",
       "4    2013  201212110DET  2012-12-11  7:30 pm     Detroit Pistons   \n",
       "\n",
       "           GuestName  total_score  total_line  game_line  Host_HostRank  ...  \\\n",
       "0          Utah Jazz        227.0       207.5        7.5             13  ...   \n",
       "1    Detroit Pistons        201.0       186.5        5.5             13  ...   \n",
       "2  San Antonio Spurs        240.0       212.0       -7.0             12  ...   \n",
       "3    New York Knicks        197.0       195.5       -3.5             12  ...   \n",
       "4     Denver Nuggets        195.0       203.5       -4.5             11  ...   \n",
       "\n",
       "   gPTS_avg10  gTS%_avg10  g3PAR_avg10  gFTr_avg10  gDRB%_avg10  gTRB%_avg10  \\\n",
       "0        99.0      0.5206       0.2230      0.2981        69.22        50.05   \n",
       "1        90.3      0.5077       0.2144      0.3095        71.46        49.48   \n",
       "2       108.0      0.5915       0.2743      0.2518        74.26        50.99   \n",
       "3       100.3      0.5473       0.3595      0.2544        74.23        47.88   \n",
       "4       101.1      0.5605       0.2173      0.3177        68.45        50.40   \n",
       "\n",
       "   gAST%_avg10  gSTL%_avg10 gBLK%_avg10 gDRtg_avg10  \n",
       "0        61.57         8.63       10.31      110.87  \n",
       "1        59.83         6.48        9.46      107.91  \n",
       "2        61.82         8.30        6.85      101.41  \n",
       "3        52.07         9.31        7.64      109.24  \n",
       "4        56.33         7.67        7.83      114.86  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Season', 'GameId', 'GameDate', 'GameTime', 'HostName', 'GuestName',\n",
       "       'total_score', 'total_line', 'game_line', 'Host_HostRank',\n",
       "       'Host_GameRank', 'Guest_GuestRank', 'Guest_GameRank', 'host_win_count',\n",
       "       'host_lose_count', 'guest_win_count', 'guest_lose_count', 'game_behind',\n",
       "       'winner', 'loser', 'host_place_streak', 'guest_place_streak',\n",
       "       'hq1_avg10', 'hq2_avg10', 'hq3_avg10', 'hq4_avg10', 'hPace_avg10',\n",
       "       'heFG%_avg10', 'hTOV%_avg10', 'hORB%_avg10', 'hFT/FGA_avg10',\n",
       "       'hORtg_avg10', 'hFG_avg10', 'hFGA_avg10', 'hFG%_avg10', 'h3P_avg10',\n",
       "       'h3PA_avg10', 'h3P%_avg10', 'hFT_avg10', 'hFTA_avg10', 'hFT%_avg10',\n",
       "       'hORB_avg10', 'hDRB_avg10', 'hTRB_avg10', 'hAST_avg10', 'hSTL_avg10',\n",
       "       'hBLK_avg10', 'hTOV_avg10', 'hPF_avg10', 'hPTS_avg10', 'hTS%_avg10',\n",
       "       'h3PAR_avg10', 'hFTr_avg10', 'hDRB%_avg10', 'hTRB%_avg10',\n",
       "       'hAST%_avg10', 'hSTL%_avg10', 'hBLK%_avg10', 'hDRtg_avg10', 'gq1_avg10',\n",
       "       'gq2_avg10', 'gq3_avg10', 'gq4_avg10', 'gPace_avg10', 'geFG%_avg10',\n",
       "       'gTOV%_avg10', 'gORB%_avg10', 'gFT/FGA_avg10', 'gORtg_avg10',\n",
       "       'gFG_avg10', 'gFGA_avg10', 'gFG%_avg10', 'g3P_avg10', 'g3PA_avg10',\n",
       "       'g3P%_avg10', 'gFT_avg10', 'gFTA_avg10', 'gFT%_avg10', 'gORB_avg10',\n",
       "       'gDRB_avg10', 'gTRB_avg10', 'gAST_avg10', 'gSTL_avg10', 'gBLK_avg10',\n",
       "       'gTOV_avg10', 'gPF_avg10', 'gPTS_avg10', 'gTS%_avg10', 'g3PAR_avg10',\n",
       "       'gFTr_avg10', 'gDRB%_avg10', 'gTRB%_avg10', 'gAST%_avg10',\n",
       "       'gSTL%_avg10', 'gBLK%_avg10', 'gDRtg_avg10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3768, 96)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014    998\n",
       "2016    985\n",
       "2015    984\n",
       "2013    801\n",
       "Name: Season, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Season.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             Utah Jazz\n",
       "1    Philadelphia 76ers\n",
       "2     San Antonio Spurs\n",
       "3       New York Knicks\n",
       "4        Denver Nuggets\n",
       "Name: winner, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.winner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "San Antonio Spurs         205\n",
       "Golden State Warriors     201\n",
       "Oklahoma City Thunder     179\n",
       "Miami Heat                175\n",
       "Los Angeles Clippers      172\n",
       "Memphis Grizzlies         160\n",
       "Cleveland Cavaliers       156\n",
       "Indiana Pacers            154\n",
       "Toronto Raptors           152\n",
       "Houston Rockets           149\n",
       "Atlanta Hawks             145\n",
       "Chicago Bulls             143\n",
       "Dallas Mavericks          132\n",
       "Portland Trail Blazers    132\n",
       "Washington Wizards        130\n",
       "Brooklyn Nets             119\n",
       "Charlotte Hornets         110\n",
       "Boston Celtics            107\n",
       "Denver Nuggets            107\n",
       "Utah Jazz                 106\n",
       "New York Knicks           106\n",
       "New Orleans Pelicans      100\n",
       "Detroit Pistons            97\n",
       "Phoenix Suns               91\n",
       "Milwaukee Bucks            87\n",
       "Sacramento Kings           83\n",
       "Minnesota Timberwolves     77\n",
       "Los Angeles Lakers         75\n",
       "Orlando Magic              63\n",
       "Philadelphia 76ers         55\n",
       "Name: winner, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.winner.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a column called 'host_wins' which will indicate whether the host team won the game or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['HostName'] == data['winner']# try this, its True/False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(data['HostName'] == data['winner']).astype(int) # try this now, with astype, you will get 1/0 for T/F, easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data['host_wins'] = (data['HostName'] == data['winner']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2239\n",
       "0    1529\n",
       "Name: host_wins, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['host_wins'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.594214\n",
       "0    0.405786\n",
       "Name: host_wins, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's look at the baseline accuracy for this data\n",
    "baseline = data['host_wins'].value_counts(normalize=True) \n",
    "# notice normalize=True in value_counts to get baseline...things are getting easier with time!\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Try to explore the data little more, always good to know your data as much as possible! You can at-least try.......**\n",
    "- use `.describe()` to find some descriptive statistics\n",
    "- create some `plots & visualizations` to better understand the shapes and relationships in our data\n",
    "- use a `correlation heatmap` to help us find variables that could predict 'host_wins'\n",
    "\n",
    "Well, I am going to skip this at the moment, you must be an expert on EDA at this stage! <br>\n",
    "**NOTE:** ***Please be aware that if you're undertaking your own data investigation, skipping EDA is a bad idea. (Obviously.)***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### STEP TWO: Set up predictor matrix (X) and target array (y).  Dummify if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# As mentioned above, I don't want some columns, let's leave them using list comprehension!\n",
    "predictors = [c for c in data.columns if c not in ['GameId','GameDate','GameTime','HostName',\n",
    "                                                   'GuestName','total_score','total_line','game_line',\n",
    "                                                   'winner','loser','host_wins','Season']]\n",
    "X = data[predictors]\n",
    "y = data['host_wins']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check:\n",
    "* to make sure we don't have any categorical predictors:\n",
    "* if we don't have any categorical predictors, then we DON'T have to dummify\n",
    "* (remember, it's normally fine to have a categorical target variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Host_HostRank</th>\n",
       "      <th>Host_GameRank</th>\n",
       "      <th>Guest_GuestRank</th>\n",
       "      <th>Guest_GameRank</th>\n",
       "      <th>host_win_count</th>\n",
       "      <th>host_lose_count</th>\n",
       "      <th>guest_win_count</th>\n",
       "      <th>guest_lose_count</th>\n",
       "      <th>game_behind</th>\n",
       "      <th>host_place_streak</th>\n",
       "      <th>...</th>\n",
       "      <th>gPTS_avg10</th>\n",
       "      <th>gTS%_avg10</th>\n",
       "      <th>g3PAR_avg10</th>\n",
       "      <th>gFTr_avg10</th>\n",
       "      <th>gDRB%_avg10</th>\n",
       "      <th>gTRB%_avg10</th>\n",
       "      <th>gAST%_avg10</th>\n",
       "      <th>gSTL%_avg10</th>\n",
       "      <th>gBLK%_avg10</th>\n",
       "      <th>gDRtg_avg10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>0.2230</td>\n",
       "      <td>0.2981</td>\n",
       "      <td>69.22</td>\n",
       "      <td>50.05</td>\n",
       "      <td>61.57</td>\n",
       "      <td>8.63</td>\n",
       "      <td>10.31</td>\n",
       "      <td>110.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>90.3</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.3095</td>\n",
       "      <td>71.46</td>\n",
       "      <td>49.48</td>\n",
       "      <td>59.83</td>\n",
       "      <td>6.48</td>\n",
       "      <td>9.46</td>\n",
       "      <td>107.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.5915</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.2518</td>\n",
       "      <td>74.26</td>\n",
       "      <td>50.99</td>\n",
       "      <td>61.82</td>\n",
       "      <td>8.30</td>\n",
       "      <td>6.85</td>\n",
       "      <td>101.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>100.3</td>\n",
       "      <td>0.5473</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>0.2544</td>\n",
       "      <td>74.23</td>\n",
       "      <td>47.88</td>\n",
       "      <td>52.07</td>\n",
       "      <td>9.31</td>\n",
       "      <td>7.64</td>\n",
       "      <td>109.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>101.1</td>\n",
       "      <td>0.5605</td>\n",
       "      <td>0.2173</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>68.45</td>\n",
       "      <td>50.40</td>\n",
       "      <td>56.33</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.83</td>\n",
       "      <td>114.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Host_HostRank  Host_GameRank  Guest_GuestRank  Guest_GameRank  \\\n",
       "0             13             21               13              22   \n",
       "1             13             21               13              23   \n",
       "2             12             20               13              22   \n",
       "3             12             20               13              21   \n",
       "4             11             24               16              22   \n",
       "\n",
       "   host_win_count  host_lose_count  guest_win_count  guest_lose_count  \\\n",
       "0               9               11               11                10   \n",
       "1              11                9                7                15   \n",
       "2               9               10               17                 4   \n",
       "3              11                8               15                 5   \n",
       "4               7               16               10                11   \n",
       "\n",
       "   game_behind  host_place_streak  ...  gPTS_avg10  gTS%_avg10  g3PAR_avg10  \\\n",
       "0         -1.5                  1  ...        99.0      0.5206       0.2230   \n",
       "1          5.0                  1  ...        90.3      0.5077       0.2144   \n",
       "2         -7.0                  2  ...       108.0      0.5915       0.2743   \n",
       "3         -3.5                  4  ...       100.3      0.5473       0.3595   \n",
       "4         -4.0                  1  ...       101.1      0.5605       0.2173   \n",
       "\n",
       "   gFTr_avg10  gDRB%_avg10  gTRB%_avg10  gAST%_avg10  gSTL%_avg10  \\\n",
       "0      0.2981        69.22        50.05        61.57         8.63   \n",
       "1      0.3095        71.46        49.48        59.83         6.48   \n",
       "2      0.2518        74.26        50.99        61.82         8.30   \n",
       "3      0.2544        74.23        47.88        52.07         9.31   \n",
       "4      0.3177        68.45        50.40        56.33         7.67   \n",
       "\n",
       "   gBLK%_avg10  gDRtg_avg10  \n",
       "0        10.31       110.87  \n",
       "1         9.46       107.91  \n",
       "2         6.85       101.41  \n",
       "3         7.64       109.24  \n",
       "4         7.83       114.86  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.info() # try this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP THREE: `Train/test split` and `StandardScaler( )`.  In this case, instead of using `train_test_split`, we're going to use the most recent season as our testing data (2016 data), and previous seasons as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before scaling, I want to see how many seasons have been played.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014    998\n",
       "2016    985\n",
       "2015    984\n",
       "2013    801\n",
       "Name: Season, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Season'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For training data, please select 2014, 2015 and 2016 seasons only.** Notice `isin()`, easy way to get things done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create your training and testing sets\n",
    "X_train = X[data['Season'].isin([2013,2014,2015])]  #or, you could use X[data['Season]<2016]\n",
    "X_test = X[data['Season'] == 2016]\n",
    "\n",
    "y_train = y[data['Season'].isin([2013,2014,2015])]\n",
    "y_test = y[data['Season'] == 2016]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here, we are scaling data after splitting, we need to scale both train and test parts, a good practice is to save the transformation from train data and use that for test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#standardise your predictor matrices\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-387bb7b1af31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;31m# need to import joblib first -- You may need to install this module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Saving the transformation, a good ML practice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scaling_transformation.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transformation saved as scaling_transformation.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "import joblib # need to import joblib first -- You may need to install this module\n",
    "\n",
    "# Saving the transformation, a good ML practice\n",
    "joblib.dump(scaler, 'scaling_transformation.pkl')\n",
    "print('transformation saved as scaling_transformation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading saved transformation \n",
    "scaler = joblib.load('scaling_transformation.pkl') \n",
    "print('Saved transformation in loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming features \n",
    "X_train_ss = scaler.transform(X_train)\n",
    "X_test_ss = scaler.transform(X_test)\n",
    "print(\"scaled features are in 'X_train_ss' and 'X_test_ss'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP FOUR: Use cross-validation to optimise the hyperparameters for your model. You might try different types of models at this stage as well, and you might use GridSearchCV (or any of the other CVs like RidgeCV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can fit a default `KNeighborsClassifier` to predict 'host_wins'.\n",
    "\n",
    "We can use cross-validation with our training data to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**set up a default KNN model and cross-validate it on the training data use 5 cross-validation folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "cross_val_score(knn, X_train_ss, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**find the mean for your cross_val_scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_accuracy = cross_val_score(knn, X_train_ss, y_train, cv=5).mean()\n",
    "print('Mean cross-validated accuracy for default knn:',knn_cv_accuracy)\n",
    "print('Baseline accuracy:', baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Our default KNN performs quite poorly** on the test data. \n",
    "* What if we changed the number of neighbors? The weighting? The distance metric?\n",
    "\n",
    "These are all hyperparameters of the KNN. How would we do this manually? We would need to evaluate on the training data the set of hyperparameters that perform best, and then use this set of hyperparameters to fit the final model and score on the testing set.\n",
    "\n",
    "#### Gridsearch pseudocode for our KNN\n",
    "\n",
    "```python\n",
    "accuracies = {}\n",
    "for k in neighbors_to_test:\n",
    "    for w in weightings_to_test:\n",
    "        for d in distance_metrics_to_test:\n",
    "            hyperparam_set = (k, w, d)\n",
    "            knn = KNeighborsClassifier(n_neighbors=n, weights=w, metric=d)\n",
    "            cv_accuracies = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "            accuracies[hyperparam_set] = np.mean(cv_accuracies)\n",
    "```\n",
    "\n",
    "In the pseudocode above, we would find the key in the dictionary (a hyperparameter set) that has the larget value (mean cross-validated accuracy).\n",
    "\n",
    "\n",
    "\n",
    "#### Using `GridSearchCV`\n",
    "\n",
    "This would be an annoying process to have to do manually. Luckily sklearn comes with a convenience class for performing gridsearch:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "```\n",
    "\n",
    "The `GridSearchCV` has a handful of important arguments:\n",
    "\n",
    "| Argument | Description |\n",
    "| --- | ---|\n",
    "| **`estimator`** | The sklearn instance of the model to fit on |\n",
    "| **`param_grid`** | A dictionary where keys are hyperparameters for the model and values are lists of values to test |\n",
    "| **`cv`** | The number of internal cross-validation folds to run for each set of hyperparameters |\n",
    "| **`n_jobs`** | How many cores to use on your computer to run the folds (-1 means use all cores) |\n",
    "| **`verbose`** | How much output to display (0 is none, 1 is limited, 2 is printouts for every internal fit) |\n",
    "\n",
    "\n",
    "Below is an example for how one might set up the gridsearch for our KNN:\n",
    "\n",
    "```python\n",
    "knn_parameters = {\n",
    "    'n_neighbors':[1,3,5,7,9],\n",
    "    'weights':['uniform','distance']\n",
    "}\n",
    "\n",
    "knn_gridsearcher = GridSearchCV(KNeighborsClassifier(), knn_parameters, cv=4, verbose=1)\n",
    "knn_gridsearcher.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Try out the sklearn gridsearch below on the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors': [5,9,15,25,40,50,60],\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['euclidean','manhattan']}\n",
    "print('Initialized parameters for Grid Search')\n",
    "print(knn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's run the GridSearch now!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_gridsearch = GridSearchCV(KNeighborsClassifier(), \n",
    "                              knn_params, \n",
    "                              n_jobs=-1, cv=5) # try verbose!\n",
    "\n",
    "print(\"GridSearch instance 'knn_gridsearch' created\\n\")\n",
    "print(\"GridSearch started............\")\n",
    "start_ = time.time()\n",
    "knn_gridsearch.fit(X_train_ss, y_train)\n",
    "time_taken = time.time()-start_\n",
    "print('Finished the GridSearch in (sec)', time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  We've fitted our GridSearch.\n",
    "\n",
    "#### Examing the results of GridSearch( )\n",
    "\n",
    "Once the gridsearch has fit (this can take awhile!) we can pull out a variety of information and useful objects from the gridsearch object, stored as attributes:\n",
    "\n",
    "| Property | Use |\n",
    "| --- | ---|\n",
    "| **`results.param_grid`** | Displays parameters searched over. |\n",
    "| **`results.best_score_`** | Best mean cross-validated score achieved. |\n",
    "| **`results.best_estimator_`** | Reference to model with best score.  Is usable / callable. |\n",
    "| **`results.best_params_`** | The parameters that have been found to perform with the best score. |\n",
    "| **`results.grid_scores_`** | Display score attributes with corresponding parameters. | \n",
    "\n",
    "**Print out the best score found in the search.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the best mean cross-validated accuracy from the gridsearch\n",
    "# hopefully this should be much better than our default mean cross-validated accuracy \n",
    "knn_gridsearch.best_score_\n",
    "\n",
    "#me\n",
    "#knn_gridsearch.best_est.........."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's have a look what was the value without GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean cross-validated accuracy for default knn:',knn_cv_accuracy) # this is default without GridSearch\n",
    "print('Baseline accuracy:',baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the set of hyperparameters that achieved the best score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out your best hyperparameters\n",
    "knn_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP FIVE: \n",
    "**Once you're happy with your hyperparameters, fit your model on your whole training data and test it on your whole testing data.  (When you use a gridsearch's `.best_estimator_`, it will already have fit a model with the best hyperparameters on your training data, so all you have to do is score it on your testing data.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign the best fit model (`best_estimator_`) to a variable and score it on the test data.**\n",
    "\n",
    "Compare this model to the baseline accuracy and your default KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign your best_estimator_ to the variable, then use .score( ) on your testing data\n",
    "best_knn = knn_gridsearch.best_estimator_\n",
    "best_knn.score(X_test_ss, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(best_knn) # try it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP SIX: \n",
    "**Then you might want to evaluate the performance of the model `(R2 score, accuracy, confusion matrix, etc)`; find the actual predictions that your model is providing and store them in a dataframe; plot your predictions against your actual target variable to visualize how well your model performed; investigate feature importance with `.coef_ if you have a parametric model.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There's lots of stuff you can do to follow up and investigate when you've found your best model,but let's just look at some different ways to assess a classifier now using what you learned in the previous lectures.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = best_knn.predict(X_test_ss)\n",
    "confusion = confusion_matrix(y_test,predictions,labels=[1,0])\n",
    "pd.DataFrame(confusion, \n",
    "             columns=['predicted_home_win','predicted_home_loss'], \n",
    "             index=['True_home_win','True_home_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall is concerned with how much you catch all the positives:\n",
    "#it's more important with cancer detection tests, for example: \n",
    "#     \"Let's make sure we RECALL all the patients with positives to do further testing!\" \n",
    "#     \"Yes!  But let's make sure we're SENSITIVE about how we tell them they might have cancer.\"\n",
    "#it's the true positives, divided by all the actual positives\n",
    "print('recall', 501/(501+93))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check that:\n",
    "print('recall using built-in method: ', recall_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision is concerned with how precisely you predict positives; in other words,\n",
    "#when you predict a positive, are you pretty sure about that prediction?\n",
    "#    \"Whhaaattttt. An email from the Home Office went to my spam filter?!\n",
    "#    \"Spam filters are great, but they damn sure better be PRECISE about what emails they think are spam!\"\n",
    "#it's the true positives, divided by all the predicted positives\n",
    "print('precision: ', 501/(501+255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check that:\n",
    "print('precision using built-in method: ', precision_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the F1 score is the harmonic mean between these two metrics:\n",
    "print('f1-score: ', f1_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkout (in pair):** How can you modify the recall or the precision?--***Recall the previous lecture when we learned to play with probabilities!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_proba = best_knn.predict_proba(X_test_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing threshold\n",
    "new_predictions = [int(p[1]>.3) for p in predictions_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test,new_predictions)# notice, the score is changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test,new_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practice'></a>\n",
    "\n",
    "## Independent practice: gridsearch regularization penalties with logistic regression\n",
    "\n",
    "---\n",
    "\n",
    "**Logistic regression models can also apply the Lasso and Ridge penalties.** The `LogisticRegression` class takes these regularization-relevant hyperparameters:\n",
    "\n",
    "| Argument | Description |\n",
    "| --- | ---|\n",
    "| **`penalty`** | `'l1'` for Lasso, `'l2'` for Ridge |\n",
    "| **`solver`** | Must be set to `'liblinear'` for the Lasso penalty to work. |\n",
    "| **`C`** | The regularization strength. Equivalent to `1./alpha` |\n",
    "\n",
    "**You should:**\n",
    "1. Fit and validate the accuracy of a default logistic regression on the basketball data.\n",
    "- Perform a gridsearch over different regularization strengths and Lasso and Ridge penalties.\n",
    "- Compare the accuracy on the test set of your optimized logistic regression to the baseline accuracy and the default model.\n",
    "- Look at the best parameters found. What was chosen? What does this suggest about our data?\n",
    "- Look at the coefficients and associated predictors for your optimized model. What appears to be the most important predictors of winning the game?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a logistic regression model, \n",
    "#and find its cross-validated scores for your training data\n",
    "#you should get accuracies higher than 0.6, which is better than KNN!\n",
    "lr = LogisticRegression()\n",
    "cross_val_score(lr, X_train_ss, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the mean of those cross-validated scores:\n",
    "cross_val_score(lr, X_train_ss, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters. \n",
    "# Use a list with 'l1' and 'l2' for the penalties,\n",
    "# Use a list with 'liblinear' for the solver,\n",
    "# Use a logspace from -3 to 0, with 50 different values\n",
    "\n",
    "# fill the dictionary of parameters\n",
    "gs_params = {'penalty':['l1','l2'],\n",
    "             'solver':['liblinear'],\n",
    "             'C':np.logspace(-3,0,50)}\n",
    "\n",
    "#create your gridsearch object\n",
    "lr_gridsearch = GridSearchCV(LogisticRegression(), \n",
    "                             gs_params, \n",
    "                             n_jobs=-1, cv=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A Good read on solvers](https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions)<br>\n",
    "[A good read on liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) the default which is a A Library for Large Linear Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit your gridsearch object on your training data\n",
    "lr_gridsearch.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best mean cross-validated score that your gridsearch found:\n",
    "# (this should be better than the mean cross-validated score for your default logistic regression above)\n",
    "lr_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best hyperparameters that your gridsearch found:\n",
    "lr_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the best estimator to a variable:\n",
    "best_lr = lr_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# score your best estimator on the testing data:\n",
    "best_lr.score(X_test_ss, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's analyse the features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to look at the coefficients\n",
    "coef_df = pd.DataFrame({'coef': best_lr.coef_[0], # its logistic regression, we can get coef_, right?\n",
    "                        'feature': X.columns,\n",
    "                        'abs_coef': np.abs(best_lr.coef_[0])})\n",
    "\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by absolute value of coefficient (magnitude)\n",
    "coef_df.sort_values('abs_coef', ascending=False, inplace=True)\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## KEY TAKEAWAYS:\n",
    "\n",
    "- You always want to use your training data to search for your best hyperparameters! You can do this with GridSearchCV, or with other sklearn objects like RidgeCV, LassoCV, ElasticNetCV, or LogisticRegressionCV.  \n",
    "\n",
    "\n",
    "- You instantiate GridSearchCV with:\n",
    "    - a model\n",
    "    - a dictionary for that model's parameters\n",
    "    - the number of cross-validation folds you want it to perform (`cv=`)\n",
    "    - how many cores to use on your computer for this job (`n_jobs=`)\n",
    "    - whether you want your model to give you some print-outs as it works (`verbose=`)\n",
    "    \n",
    "\n",
    "- Once you've instantiated the GridSearch object, you can fit it on your training data\n",
    "\n",
    "\n",
    "- Once it's finished searching, you can access some useful attributes:\n",
    "    - `.best_score_`, to find the mean cross-validated score of the best estimator\n",
    "    - `.best_params_`, to find the best hyperparameters \n",
    "    - `.best_estimator_`, which you will assign to a variable in order to use `.score()`, `.predict()`, `.coef_`, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PROGRESS CHECKPOINT:\n",
    "\n",
    "- EVERYONE should be able to:\n",
    "    - give an example of a hyperparameter\n",
    "    - explain the basic intuition behind how GridSearch works\n",
    "    - copy and paste the GridSearching from this lesson and adapt it to a different lab to find the best hyperparameters for a model\n",
    "\n",
    "\n",
    "\n",
    "- MANY of you should be able to:\n",
    "    -  use your best estimator from the gridsearch to evaluate model effectiveness and investigate feature importance\n",
    "    -  come up with reasonable parameter dictionaries by yourself for KNN and Logistic Regression\n",
    "    -  create a confusion matrix using your predictions from your best estimator, and understand how to interpret it\n",
    "\n",
    "\n",
    "- SOME of you should be able to:\n",
    "    - look at the documentation for GridSearchCV on sklearn's website, and investigate the different options that are available (for example, the attribute `.cv_results_`)\n",
    "    - create a ROC curve and a Precision-Recall curve for your predictions, and be able to interpret them\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
